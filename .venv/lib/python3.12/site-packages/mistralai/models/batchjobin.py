"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .apiendpoint import APIEndpoint
from .batchrequest import BatchRequest, BatchRequestTypedDict
from mistralai.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Dict, List, Optional
from typing_extensions import NotRequired, TypedDict


class BatchJobInTypedDict(TypedDict):
    endpoint: APIEndpoint
    input_files: NotRequired[Nullable[List[str]]]
    r"""The list of input files to be used for batch inference, these files should be `jsonl` files, containing the input data corresponding to the bory request for the batch inference in a \"body\" field. An example of such file is the following: ```json {\"custom_id\": \"0\", \"body\": {\"max_tokens\": 100, \"messages\": [{\"role\": \"user\", \"content\": \"What is the best French cheese?\"}]}} {\"custom_id\": \"1\", \"body\": {\"max_tokens\": 100, \"messages\": [{\"role\": \"user\", \"content\": \"What is the best French wine?\"}]}} ```"""
    requests: NotRequired[Nullable[List[BatchRequestTypedDict]]]
    model: NotRequired[Nullable[str]]
    r"""The model to be used for batch inference."""
    agent_id: NotRequired[Nullable[str]]
    r"""In case you want to use a specific agent from the **deprecated** agents api for batch inference, you can specify the agent ID here."""
    metadata: NotRequired[Nullable[Dict[str, str]]]
    r"""The metadata of your choice to be associated with the batch inference job."""
    timeout_hours: NotRequired[int]
    r"""The timeout in hours for the batch inference job."""


class BatchJobIn(BaseModel):
    endpoint: APIEndpoint

    input_files: OptionalNullable[List[str]] = UNSET
    r"""The list of input files to be used for batch inference, these files should be `jsonl` files, containing the input data corresponding to the bory request for the batch inference in a \"body\" field. An example of such file is the following: ```json {\"custom_id\": \"0\", \"body\": {\"max_tokens\": 100, \"messages\": [{\"role\": \"user\", \"content\": \"What is the best French cheese?\"}]}} {\"custom_id\": \"1\", \"body\": {\"max_tokens\": 100, \"messages\": [{\"role\": \"user\", \"content\": \"What is the best French wine?\"}]}} ```"""

    requests: OptionalNullable[List[BatchRequest]] = UNSET

    model: OptionalNullable[str] = UNSET
    r"""The model to be used for batch inference."""

    agent_id: OptionalNullable[str] = UNSET
    r"""In case you want to use a specific agent from the **deprecated** agents api for batch inference, you can specify the agent ID here."""

    metadata: OptionalNullable[Dict[str, str]] = UNSET
    r"""The metadata of your choice to be associated with the batch inference job."""

    timeout_hours: Optional[int] = 24
    r"""The timeout in hours for the batch inference job."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "input_files",
            "requests",
            "model",
            "agent_id",
            "metadata",
            "timeout_hours",
        ]
        nullable_fields = ["input_files", "requests", "model", "agent_id", "metadata"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
